---
title: "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator"
author: "Yigong Hu"
date: "2023-03-25"
image: HGWR_Illustrate.jpg
execute: 
    cache: true
bibliography: references.bib
---

Hierarchical and geographically weighted regression (HGWR) is a spatial modelling method designed for data with a spatially hierarchical structure,
i.e., samples are grouped by their locations.
Variables are divied into group level and sample level.
It calibrate three types of effects: local fixed effects, global fixed effects, and random effects.
Only group level variables can be local fixed effects.
In this post, the usage and some examples are going to be shown with R code.

# Installation

The HGWR model is implemented with C++ and R codes.
And we have published an R package **hgwrr** to provice easy-to-use user interfaces to build HGWR moddels.

## From CRAN

Install the **hgwrr** package from CRAN is very easy, through

```r
install.packages("hgwrr")
```

Note if you are using Linux or macOS platforms, this package requires GSL to be installed and can be found by R.
Please install it using your package manager, like `apt`, `dnf`, and `brew`.
On Windows, the pre-built binary package would be provided by CRAN.

## From Source Code

Please download the [R source package (v0.3.0)] and install it via the following code

```bash
R CMD INSTALL hgwrr_0.3-0.tar.gz
```

Please make sure that the following dependencies are already installed in your R environment:

- Armadillo
- GSL

Just the package-manager versions are enough.

# Usage

An HGWR model can be calibrated using the following function,

```r
hgwr(
  formula, data, local.fixed, coords, bw,
  alpha = 0.01, eps_iter = 1e-06, eps_gradient = 1e-06, max_iters = 1e+06,
  max_retries = 1e+06, ml_type = HGWR_ML_TYPE_D_ONLY, verbose = 0
)
```

There seems to be quite a few arguments, but most of them have default values which would be fine on most ocassions.
The first five arguments are mandatory.

`formula`
: This argument accepts a formula object in R. Its format follows lme4 package.
As there are two types of effects: fixed effects and random effects,
we use the following format to specify both of them
    
    ```r
    dependent ~ fixed1 + fixed2 + (random1 + random2 | group)
    ```

`data`
: It accepts a DataFrame object in R. All variables specified in formula are extracted from data.
In this stage, `Spatial*DataFrame` is not supported, and will not be supported in the future.

`local.fixed`
: It accepts a list of character specifying which fixed effects are local.
For example, if `fixed1` needs to be locally fixed, then set `local.fixed` to `c("fixed1")`.

`coords`
: It accepts a matrix of 2 columns. Each row is the longitude and latitude of each group.

`bw`
: It accepts an integer or numeric number to specify the bandwidth used in geographically weighted process.
Currently, it can only be adaptive bandwidth.

For other arguments, if the default values cause some problems, and you want to change some of them,
please check the documentation of function hgwr() for more infomation.

# Simulation Experiment

To carry out the experiment, there are some packages required to be in your R environment.
Please install them if they are abscent.

- [tidyverse](https://cran.r-project.org/web/packages/tidyverse/index.html)
- [ggpmisc](https://cran.r-project.org/web/packages/ggpmisc/index.html)
- [ggpubr](https://cran.r-project.org/web/packages/ggpubr/index.html)
- [Metrics](https://cran.r-project.org/web/packages/Metrics/index.html)
- [sf](https://cran.r-project.org/web/packages/sf/index.html)
- [MASS](https://cran.r-project.org/web/packages/MASS/index.html)
- [GWmodel](https://cran.r-project.org/web/packages/GWmodel/index.html)
- [lmerTest](https://cran.r-project.org/web/packages/lmerTest/index.html)
- [performance](https://cran.r-project.org/web/packages/performance/index.html)

The CRAN versions now are fine.

```{r}
#| message: false
#| warning: false

library(tidyverse)
library(ggpubr)
library(Metrics)
library(sf)
library(hgwrr)
library(GWmodel)
library(lmerTest)
```


## Data Generation

The data used in this experiment is generated by the following code.
This process is inspired by @FotheringhamYang-2017.

```{r}
nu <- 25
nv <- 25
ncoords <- nu * nv
coords <- matrix(c(rep(seq_len(nu) - 1, each = 25), rep(seq_len(nv) - 1, times = 25)), ncol = 2) %>% as.data.frame()
colnames(coords) <- c("u", "v")
u <- coords[["u"]]
v <- coords[["v"]]
g1 <- (36 - (6 - u/2)^2)*(36 - (6 - v/2)^2)/324
g2 <- 4 * exp(- (scale(u)^2 + scale(v)^2)/2)
h1 <- rep(2, times = ncoords)
set.seed(648)
z1 <- rnorm(ncoords, mean = 2, sd = 0.5)
set.seed(12574)
b0 <- (u + v) / 12 - 2 + rnorm(ncoords, mean = 2, sd = 0.2)
betas <- data.frame(Intercept = b0, g1, g2, h1, z1)
set.seed(648)
nsamples <- floor(runif(ncoords, 20, 50))
iloc <- rep(seq_len(ncoords), times = nsamples)
set.seed(1)
x <- MASS::mvrnorm(sum(nsamples), mu = rep(0, 4), Sigma = diag(4))
x[,1] <- aggregate(x[,1], by = list(iloc), FUN = mean)[["x"]] %>% rep(times = nsamples)
x[,2] <- aggregate(x[,2], by = list(iloc), FUN = mean)[["x"]] %>% rep(times = nsamples)
colnames(x) <- c("g1", "g2", "h1", "z1")
set.seed(2)
e <- rnorm(sum(nsamples))
y <- rowSums(cbind(1, x) * as.matrix(betas[iloc,])) + e
data <- cbind(y = y, x, group = iloc) %>% as.data.frame()
sim <- list(
    data = data,
    betas = betas,
    coords = coords
)
sim_sf <- st_as_sf(with(sim, cbind(data, coords[data$group,])), coords = c("u", "v"))
glimpse(sim)
```

With this data, we are going to calibrate a HGWR model,
geographically weighted regression [GWR, @BrunsdonFotheringham-1996] model,
multiscale geographically weighted regression [MGWR, the PSDM implementation, @LuBrunsdon-2017] model,
and hierarchical linear model [HLM, @Raudenbush-1993].
The MGWR model requires a very large amout of memory and time,
so we only demonstrate the workable codes here.
The result we got on a high-performance computing platform is used instead.

## Model Calibration

### HGWR

A HGWR model is calibrated with the following code.

```{r sim-hgwr-model}
hgwr_formula <- y ~ g1 + g2 + h1 + (z1 | group)
hgwr_model <- hgwr(
    hgwr_formula, sim$data, c("g1", "g2"), sim$coords, "CV", 
    kernel = "bisquared",
)
summary(hgwr_model)
```

### GWR

```{r sim-gwr-model}
sim_sp <- as(sim_sf, Class = "Spatial")
gwr_formula <- y ~ g1 + g2 + h1 + z1
gwr_bw <- bw.gwr(gwr_formula, sim_sp, approach = "AIC", adaptive = T, kernel = "bisquare")
gwr_model <- gwr.basic(gwr_formula, sim_sp, bw = gwr_bw, adaptive = T, kernel = "bisquare")
gwr_model
```

### HLM

```{r sim-hlm-model}
#| results: hold

hlm_model <- lmerTest::lmer(y ~ g1 + g2 + h1 + z1 + (z1 | group), sim$data)
summary(hlm_model)
performance::r2(hlm_model)
```

### MGWR

```{r sim-mgwr-model}
#| eval: false

mgwr_model <- gwr.multiscale(y ~ g1 + g2 + h1 + z1, sim_sp, adaptive = T)
mgwr_model
```

```{r sim-mgwr-model-load}
#| echo: false

load("./results/model_mgwr.rda")
mgwr_model
```

## Estimate Analysis

As the actual values of coefficients are already known,
we can compare the performance of these models by comparing the closeness between their estimates and actual values.

```{r}
#| code-fold: true
#| code-summary: Create some handy variables.

coef_names <- c("Intercept", "g1", "g2", "h1", "z1")
coef_names_plot <- c("Intercept", "g1", "g2", "z1")
coef_name_map <- list(
    Intercept = "alpha[0]",
    g1 = "gamma[1]",
    g2 = "gamma[2]",
    h1 = "beta[1]",
    x1 = "beta[1]",
    z1 = "mu[1]"
)
coef_name_labels <- list(
    Intercept = bquote(alpha[0]),
    g1 = bquote(gamma[1]),
    g2 = bquote(gamma[2]),
    h1 = bquote(beta[1]),
    x1 = bquote(beta[1]),
    z1 = bquote(mu[1])
)
models_name <- c("GWR", "MGWR", "HLM", "HGWR")
```

Firstly, we collect coefficient estimates.

```{r}
### Real values
beta0 <- sim$betas[coef_names]
### HGWR
hgwr_betas <- coef(hgwr_model)[coef_names]
### GWR
gwr_betas <- gwr_model$SDF@data[coef_names] %>%
    aggregate(by = list(sim$data$group), FUN = mean) %>%
    as.tibble() %>%
    select(all_of(coef_names))
### MGWR
mgwr_betas <- mgwr_model$SDF@data[coef_names] %>%
    aggregate(by = list(sim$data$group), FUN = mean) %>%
    as.tibble() %>%
    select(all_of(coef_names))
### HLM
hlm_betas <- coef(hlm_model)$group
colnames(hlm_betas)[1] <- "Intercept"
hlm_betas <- hlm_betas[coef_names]
models_coef <- map(models_name, ~ get(paste0(tolower(.x), "_betas")))
names(models_coef) <- models_name
models_coef_real <- c(list(Real = beta0), models_coef)
```

Then, plot estimate values at their location and the real values to see their distributions.

```{r}
#| fig-width: 10
#| fig-height: 10.5

models_coef_real %>%
    map_dfr(function(x) {
        select(x, all_of(coef_names)) %>%
            map_dfr(~ data.frame(Value = .x, sim$coords), .id = "Coefficient")
    }, .id = "Algorithm") %>% 
    mutate(Algorithm = ordered(as.factor(Algorithm), names(models_coef_real)),
           Coefficient = ordered(as.factor(Coefficient), coef_names, 
                                 labels = coef_name_map[coef_names])) %>%
    ggplot(aes(u, v, fill = Value)) + geom_raster() + 
        scale_x_continuous(expand = expansion()) +
        scale_y_continuous(expand = expansion()) +
        scale_fill_gradient2(limits = c(0, 4), low = "#000099", 
                             mid = "#CCFF33", high = "#CC0033",
                             midpoint = 2, oob = scales::squish) +
        facet_grid(rows = vars(Algorithm), cols = vars(Coefficient), labeller = label_parsed) +
        coord_fixed() + theme_bw() +
        theme(legend.position = "bottom", legend.key.height = unit(10, "pt")) 
```

A scatter plot along with it may be useful.

```{r}
#| fig-width: 10
#| fig-height: 9

beta_hat_real <- models_coef %>%
    map_dfr(function(x) {
        map_dfr(coef_names, ~ data.frame(Coefficient = .x, Estimated = x[[.x]], Real = beta0[[.x]]))
    }, .id = "Algorithm") %>%
    mutate(Algorithm = ordered(as.factor(Algorithm), names(models_coef)),
           Coefficient = ordered(as.factor(Coefficient), coef_names, labels = coef_name_map[coef_names]))
beta_hat_rmse <- beta_hat_real %>%
    group_nest(Algorithm, Coefficient) %>%
    mutate(rmse = map(data, ~ sqrt(mean((.x$Estimated - .x$Real)^2))),
           data = NULL)
ggplot(beta_hat_real, aes(x = Real, y = Estimated)) +
    geom_point() + geom_abline(intercept = 0, slope = 1) +
    geom_text(aes(x = -Inf, y = Inf, label = sprintf("RMSE=%.3f", rmse)),
              data = beta_hat_rmse, hjust = 0, vjust = 1.2) +
    scale_y_continuous(limits = c(-5, 10), oob = scales::squish) +
    facet_grid(rows = vars(Algorithm), cols = vars(Coefficient), labeller = label_parsed) +
    theme_bw()
```

From these two figures, we can find that:

1. In the results of GWR, spatial heterogeneity is revealed in estimates for all variables.
Although $\hat{\beta}_1$ should be constant across the study area,
GWR still generate spatially varying estimates for it.
This is a kind of over-fitting from the spatial perspective.
Besides, as the bandwidth is small, estimates for $\gamma_1$ and $\gamma_2$ are too local.
Consequently, there are quite a few outliers disrupting the spatial trend.
2. MGWR partly gets over issues of GWR by adopting parameter-specified bandwidths,
instead of a uniform bandwidth.
And it performs better when estimating $\gamma_1$ and $\gamma_2$.
For global fixed effects, MGWR still generates spatially varying estimates,
but they vary much slightly than estimates from GWR.
For random effects, the results are slightly smoothed as well.
MGWR also borrow a few points from neighbors.
There is another serious problem in MGWR that it requires too much computing time and memory.
3. In the results of HLM, there is only one estimate for $\beta_1$ across the whole area.
And estimates for $\mu_1$, The problem lies in estimates for $\gamma_1$ and $\gamma_2$.
As they are fixed effects in HLM, their estimates are also constant for all samples.
However, spatial heterogeneity is expected in them.
4. HGWR is the final solution.
For global fixed effects, it generates globally constant estimates for all samples.
For random effects, it would not smooth the estimates because they are not obtained by borrowing points.
And for local fixed effects, we can discover spatial heterogeneity from their estimates.
And it would not repeat computation for samples at each location.
Thus, only the number of locations obviously affects the computation efficiency.
This can reduce a large amount of computing time and memory.

There is an animation demonstrating the problem about bandwidth we addressed above.

{{< video https://hpdell.github.io/hgwr/assets/multisampling.mp4 
    title='Issues related to the bandwidth in hierarchical spatial data'
>}}

As shown in this video, bandwidths have inequal spatial scale for two samples (represented by cubes).
Both the samples represented by large red cubes and large blue cubes take 41 neighbour samples to calibrate GWR models.
For the red one, neighbours on 8 nearest locations are taken. But the figure for the blue one is only 6.
This situation means estimated coefficients are more smoothed for the red samples.
In other words, estimations for the blue samples are much local.

HGWR overcome this drawback by introducing hierarchical structure with a special designed backfitting estimator.
With the popularity of spatiotemporal big data,
situations wherein the specific parameters for which HGWR was optimized are becoming more prevalent,
suggesting that HGWR holds considerable promise as a useful tool for analyzing such data sets.

## Estimation Errors

We can use some indicators to evaulate estimation errors for each model and coefficient.
The following code generate a box-plot of absolute coefficient errors $\mathrm{AE}$, which is
$$
\mathrm{AE}_i = \left| r_i - e_i \right|
$$
where $r_i$ reprsents the real value at sample $i$, and $e_i$ represents the corresponding estimate.

```{r}
models_coef %>%
    map(~ select(.x, all_of(coef_names))) %>%
    map_dfr(function(x) {
        map2_dfr(x, names(x), ~ data.frame(ae = ae(beta0[[.y]], .x)), .id = "Coefficient")
    }, .id = "Algorithm") %>%
    mutate(Algorithm = ordered(as.factor(Algorithm), names(models_coef)),
           Coefficient = ordered(as.factor(Coefficient), coef_names)) %>%
    arrange(Coefficient, Algorithm) %>%
    ggplot(aes(x = Coefficient, y = ae, fill = Algorithm)) + geom_boxplot() +
        scale_y_log10(name = 'Squared Error', labels = ~ sprintf("%g", .x)) +
        scale_x_discrete(labels = coef_name_labels[coef_names]) +
        theme_bw() +
        theme(legend.position = "top", axis.text.y = element_text(angle = 90, hjust = 0.5))
```

And the following code generate a bar-plot showing root mean squared errors $\mathrm{RMSE}$ of each coefficient, which is
$$
\mathrm{RMSE} = \sum_{i=1}^n \left(r_i-e_i\right)^2
$$
where $r_i$ reprsents the real value at sample $i$, and $e_i$ represents the corresponding estimate.

```{r}
models_coef %>%
    map_dfr(function (alg) {
        map_dfr(coef_names, function (coef, real) {
            est_rmse <- rmse(real[[coef]], alg[[coef]])
            data.frame(Coefficient = coef, Value = est_rmse)
        }, beta0)
    }, .id = "Algorithm") %>%
    mutate(Algorithm = ordered(as.factor(Algorithm), c("GWR", "MGWR", "HLM", "HGWR")),
           Coefficient = ordered(as.factor(Coefficient), coef_names)) %>%
    ggplot(aes(x = Coefficient, y = Value, fill = Algorithm)) + 
        geom_bar(stat = 'identity', position = position_dodge()) + ylab("RMSE") +
        scale_y_continuous(limits = c(0, 1.6), oob = scales::squish, expand = expansion()) +
        scale_x_discrete(labels = coef_name_labels[coef_names]) +
        theme_bw() + theme(legend.position = "top")
```

From the perspective of estimation errors, HGWR significantly reduces the estimation error for local fixed effects.
It can get over the affect of global fixed effects and random effects.

# Summary

In this post we introduced how to calibrate an HGWR model using R package **hgwrr**.
It is demonstrated that HGWR could properly estimate local fixed effects,
global fixed effects, and random effects simultaneously.
HGWR could usually successfully distinguish local fixed effects from other effect types.
For local fixed effects, spatial heterogeneity is considered as with GWR;
moreover, global fixed effects and random effects are estimated as accurately as when using HLM.
Thus, HGWR can be regarded as a successful combination of GWR and HLM.
