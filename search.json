[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/HGWR/index.html",
    "href": "posts/HGWR/index.html",
    "title": "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator",
    "section": "",
    "text": "Hierarchical and geographically weighted regression (HGWR) is a spatial modelling method designed for data with a spatially hierarchical structure, i.e., samples are grouped by their locations. Variables are divied into group level and sample level. It calibrate three types of effects: local fixed effects, global fixed effects, and random effects. Only group level variables can be local fixed effects. In this post, the usage and some examples are going to be shown with R code."
  },
  {
    "objectID": "posts/HGWR/index.html#from-cran",
    "href": "posts/HGWR/index.html#from-cran",
    "title": "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator",
    "section": "From CRAN",
    "text": "From CRAN\nInstall the hgwrr package from CRAN is very easy, through\ninstall.packages(\"hgwrr\")\nNote if you are using Linux or macOS platforms, this package requires GSL to be installed and can be found by R. Please install it using your package manager, like apt, dnf, and brew. On Windows, the pre-built binary package would be provided by CRAN."
  },
  {
    "objectID": "posts/HGWR/index.html#from-source-code",
    "href": "posts/HGWR/index.html#from-source-code",
    "title": "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator",
    "section": "From Source Code",
    "text": "From Source Code\nPlease download the [R source package (v0.3.0)] and install it via the following code\nR CMD INSTALL hgwrr_0.3-0.tar.gz\nPlease make sure that the following dependencies are already installed in your R environment:\n\nArmadillo\nGSL\n\nJust the package-manager versions are enough."
  },
  {
    "objectID": "posts/HGWR/index.html#data-generation",
    "href": "posts/HGWR/index.html#data-generation",
    "title": "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator",
    "section": "Data Generation",
    "text": "Data Generation\nThe data used in this experiment is generated by the following code.\n\nnu <- 25\nnv <- 25\nncoords <- nu * nv\ncoords <- matrix(c(rep(seq_len(nu) - 1, each = 25), rep(seq_len(nv) - 1, times = 25)), ncol = 2) %>% as.data.frame()\ncolnames(coords) <- c(\"u\", \"v\")\nu <- coords[[\"u\"]]\nv <- coords[[\"v\"]]\ng1 <- (36 - (6 - u/2)^2)*(36 - (6 - v/2)^2)/324\ng2 <- 4 * exp(- (scale(u)^2 + scale(v)^2)/2)\nh1 <- rep(2, times = ncoords)\nset.seed(648)\nz1 <- rnorm(ncoords, mean = 2, sd = 0.5)\nset.seed(12574)\nb0 <- (u + v) / 12 - 2 + rnorm(ncoords, mean = 2, sd = 0.2)\nbetas <- data.frame(Intercept = b0, g1, g2, h1, z1)\nset.seed(648)\nnsamples <- floor(runif(ncoords, 20, 50))\niloc <- rep(seq_len(ncoords), times = nsamples)\nset.seed(1)\nx <- MASS::mvrnorm(sum(nsamples), mu = rep(0, 4), Sigma = diag(4))\nx[,1] <- aggregate(x[,1], by = list(iloc), FUN = mean)[[\"x\"]] %>% rep(times = nsamples)\nx[,2] <- aggregate(x[,2], by = list(iloc), FUN = mean)[[\"x\"]] %>% rep(times = nsamples)\ncolnames(x) <- c(\"g1\", \"g2\", \"h1\", \"z1\")\nset.seed(2)\ne <- rnorm(sum(nsamples))\ny <- rowSums(cbind(1, x) * as.matrix(betas[iloc,])) + e\ndata <- cbind(y = y, x, group = iloc) %>% as.data.frame()\nsim <- list(\n    data = data,\n    betas = betas,\n    coords = coords\n)\nsim_sf <- st_as_sf(with(sim, cbind(data, coords[data$group,])), coords = c(\"u\", \"v\"))\nglimpse(sim)\n\nList of 3\n $ data  :'data.frame': 21434 obs. of  6 variables:\n  ..$ y    : num [1:21434] 4.274 0.952 -2.278 1.21 4.547 ...\n  ..$ g1   : num [1:21434] -0.154 -0.154 -0.154 -0.154 -0.154 ...\n  ..$ g2   : num [1:21434] 0.109 0.109 0.109 0.109 0.109 ...\n  ..$ h1   : num [1:21434] 3.169 -0.031 -1.092 -0.983 1.719 ...\n  ..$ z1   : num [1:21434] -0.626 0.184 -0.836 1.595 0.33 ...\n  ..$ group: num [1:21434] 1 1 1 1 1 1 1 1 1 1 ...\n $ betas :'data.frame': 625 obs. of  5 variables:\n  ..$ Intercept: num [1:625] 0.35 0.429 0.33 0.267 0.465 ...\n  ..$ g1       : num [1:625] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ g2       : num [1:625] 0.252 0.314 0.384 0.461 0.543 ...\n  ..$ h1       : num [1:625] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ z1       : num [1:625] 2.46 2.63 1.25 1.91 1.91 ...\n $ coords:'data.frame': 625 obs. of  2 variables:\n  ..$ u: num [1:625] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ v: num [1:625] 0 1 2 3 4 5 6 7 8 9 ...\n\n\nWith this data, we are going to calibrate a HGWR, GWR, and HLM models. The multiscale GWR (MGWR) requires a very large amout of memory and time, so we only demonstrate the workable codes here. The result we got on a high-performance computing platform is used instead."
  },
  {
    "objectID": "posts/HGWR/index.html#model-calibration",
    "href": "posts/HGWR/index.html#model-calibration",
    "title": "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator",
    "section": "Model Calibration",
    "text": "Model Calibration\n\nHGWR\nA HGWR model is calibrated with the following code.\n\nhgwr_formula <- y ~ g1 + g2 + h1 + (z1 | group)\nhgwr_model <- hgwr(\n    hgwr_formula, sim$data, c(\"g1\", \"g2\"), sim$coords, \"CV\", \n    kernel = \"bisquared\",\n)\nsummary(hgwr_model)\n\nHierarchical and geographically weighted regression model\n=========================================================\nFormula: hgwr_formula\n Method: Back-fitting and Maximum likelihood\n   Data: sim$data\n\nDiagnostics\n-----------\n Rsquared \n 0.908050 \n\nScaled residuals\n----------------\n       Min         1Q    Median        3Q       Max \n -3.995269  -0.648274  0.000561  0.652537  3.530948 \n\nOther Information\n-----------------\nNumber of Obs: 21434\n       Groups: group , 625\n\n\n\n\nGWR\n\nsim_sp <- as(sim_sf, Class = \"Spatial\")\ngwr_formula <- y ~ g1 + g2 + h1 + z1\ngwr_bw <- bw.gwr(gwr_formula, sim_sp, approach = \"AIC\", adaptive = T, kernel = \"bisquare\")\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth (number of nearest neighbours): 13254 AICc value: 68302.68 \nAdaptive bandwidth (number of nearest neighbours): 8199 AICc value: 67235.53 \nAdaptive bandwidth (number of nearest neighbours): 5074 AICc value: 66677.95 \nAdaptive bandwidth (number of nearest neighbours): 3143 AICc value: 66381.08 \nAdaptive bandwidth (number of nearest neighbours): 1949 AICc value: 66152.06 \nAdaptive bandwidth (number of nearest neighbours): 1212 AICc value: 65879.63 \nAdaptive bandwidth (number of nearest neighbours): 755 AICc value: 65553.06 \nAdaptive bandwidth (number of nearest neighbours): 474 AICc value: 65089.52 \nAdaptive bandwidth (number of nearest neighbours): 299 AICc value: 64121.17 \nAdaptive bandwidth (number of nearest neighbours): 192 AICc value: 113016 \nAdaptive bandwidth (number of nearest neighbours): 366 AICc value: 64842.55 \nAdaptive bandwidth (number of nearest neighbours): 258 AICc value: 63736.27 \nAdaptive bandwidth (number of nearest neighbours): 232 AICc value: 63688.32 \nAdaptive bandwidth (number of nearest neighbours): 216 AICc value: 64235.33 \nAdaptive bandwidth (number of nearest neighbours): 241 AICc value: 63684.82 \nAdaptive bandwidth (number of nearest neighbours): 248 AICc value: 63701.3 \nAdaptive bandwidth (number of nearest neighbours): 238 AICc value: 63682.91 \nAdaptive bandwidth (number of nearest neighbours): 235 AICc value: 63681.33 \nAdaptive bandwidth (number of nearest neighbours): 234 AICc value: 63681.33 \n\ngwr_model <- gwr.basic(gwr_formula, sim_sp, bw = gwr_bw, adaptive = T, kernel = \"bisquare\")\ngwr_model\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-03-30 11:46:44 \n   Call:\n   gwr.basic(formula = gwr_formula, data = sim_sp, bw = gwr_bw, \n    kernel = \"bisquare\", adaptive = T)\n\n   Dependent (y) variable:  y\n   Independent variables:  g1 g2 h1 z1\n   Number of data points: 21434\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-5.6729 -0.9458 -0.0075  0.9499  5.3801 \n\n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) 2.002434   0.009684  206.78   <2e-16 ***\n   g1          1.659038   0.057600   28.80   <2e-16 ***\n   g2          1.791212   0.056544   31.68   <2e-16 ***\n   h1          2.010065   0.009660  208.09   <2e-16 ***\n   z1          2.042761   0.009661  211.44   <2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 1.417 on 21429 degrees of freedom\n   Multiple R-squared: 0.8064\n   Adjusted R-squared: 0.8064 \n   F-statistic: 2.232e+04 on 4 and 21429 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 43019.04\n   Sigma(hat): 1.416769\n   AIC:  75771.36\n   AICc:  75771.37\n   BIC:  54445.03\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidth: 234 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                 Min.  1st Qu.   Median  3rd Qu.    Max.\n   Intercept -0.36804  1.37847  1.98864  2.63158  3.8950\n   g1        -5.46208  0.37769  1.59423  2.78590 11.7942\n   g2        -5.90918  0.79411  1.74163  2.85702 13.1842\n   h1         1.51922  1.94629  2.01310  2.07438  2.3601\n   z1         1.14097  1.83026  2.01883  2.21836  2.8169\n   ************************Diagnostic information*************************\n   Number of data points: 21434 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1348.983 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 20085.02 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 63681.33 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 62482 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 50756.14 \n   Residual sum of squares: 22014.71 \n   R-square value:  0.9009503 \n   Adjusted R-square value:  0.8942974 \n\n   ***********************************************************************\n   Program stops at: 2023-03-30 11:47:26 \n\n\n\n\nHLM\n\nhlm_model <- lmerTest::lmer(y ~ g1 + g2 + h1 + z1 + (z1 | group), sim$data)\nsummary(hlm_model)\nperformance::r2(hlm_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ g1 + g2 + h1 + z1 + (z1 | group)\n   Data: sim$data\n\nREML criterion at convergence: 64443.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9791 -0.6478  0.0011  0.6521  3.5484 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 0.7750   0.8804       \n          z1          0.2511   0.5011   0.01\n Residual             1.0087   1.0044       \nNumber of obs: 21434, groups:  group, 625\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(>|t|)    \n(Intercept) 1.991e+00  3.595e-02 6.202e+02  55.377  < 2e-16 ***\ng1          1.581e+00  2.093e-01 6.230e+02   7.552 1.53e-13 ***\ng2          1.805e+00  1.988e-01 6.261e+02   9.079  < 2e-16 ***\nh1          2.014e+00  7.043e-03 2.037e+04 286.022  < 2e-16 ***\nz1          2.026e+00  2.136e-02 6.228e+02  94.830  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n   (Intr) g1     g2     h1    \ng1  0.007                     \ng2  0.021 -0.041              \nh1  0.000 -0.001  0.000       \nz1  0.007  0.001  0.000  0.002\n# R2 for Mixed Models\n\n  Conditional R2: 0.902\n     Marginal R2: 0.803\n\n\n\n\nMGWR\n\nmgwr_model <- gwr.multiscale(y ~ g1 + g2 + h1 + z1, sim_sp, adaptive = T)\nmgwr_model\n\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-01-07 00:33:30 \n   Call:\n   gwr.multiscale(formula = y ~ g1 + g2 + h1 + z1, data = sim_sp, \n    adaptive = T, hatmatrix = F, parallel.method = \"omp\", parallel.arg = 48)\n\n   Dependent (y) variable:  y\n   Independent variables:  g1 g2 h1 z1\n   Number of data points: 21434\n   ***********************************************************************\n   *                       Multiscale (PSDM) GWR                          *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidths for each coefficient(number of nearest neighbours): \n              (Intercept)    g1    g2    h1 z1\n   Bandwidth          382  1054   474 19875 62\n\n   ****************Summary of GWR coefficient estimates:******************\n                  Min.   1st Qu.    Median   3rd Qu.   Max.\n   Intercept  0.244383  1.451943  1.998243  2.611283 3.8138\n   g1        -0.072043  0.746596  1.426176  2.567027 4.2021\n   g2        -1.955412  0.969786  1.738936  2.659654 4.9470\n   h1         2.003644  2.010011  2.015701  2.019937 2.0227\n   z1         0.287334  1.691472  2.039915  2.376235 3.5291\n   ************************Diagnostic information*************************\n\n   ***********************************************************************\n   Program stops at: 2023-01-07 04:58:17"
  },
  {
    "objectID": "posts/HGWR/index.html#estimate-analysis",
    "href": "posts/HGWR/index.html#estimate-analysis",
    "title": "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator",
    "section": "Estimate Analysis",
    "text": "Estimate Analysis\nAs the actual values of coefficients are already known, we can compare the performance of these models by comparing the closeness between their estimates and actual values.\n\n\nCreate some handy variables.\ncoef_names <- c(\"Intercept\", \"g1\", \"g2\", \"h1\", \"z1\")\ncoef_names_plot <- c(\"Intercept\", \"g1\", \"g2\", \"z1\")\ncoef_name_map <- list(\n    Intercept = \"alpha[0]\",\n    g1 = \"gamma[1]\",\n    g2 = \"gamma[2]\",\n    h1 = \"beta[1]\",\n    x1 = \"beta[1]\",\n    z1 = \"mu[1]\"\n)\ncoef_name_labels <- list(\n    Intercept = bquote(alpha[0]),\n    g1 = bquote(gamma[1]),\n    g2 = bquote(gamma[2]),\n    h1 = bquote(beta[1]),\n    x1 = bquote(beta[1]),\n    z1 = bquote(mu[1])\n)\nmodels_name <- c(\"GWR\", \"MGWR\", \"HLM\", \"HGWR\")\n\n\nFirstly, we collect coefficient estimates.\n\n### Real values\nbeta0 <- sim$betas[coef_names]\n### HGWR\nhgwr_betas <- coef(hgwr_model)[coef_names]\n### GWR\ngwr_betas <- gwr_model$SDF@data[coef_names] %>%\n    aggregate(by = list(sim$data$group), FUN = mean) %>%\n    as.tibble() %>%\n    select(all_of(coef_names))\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n### MGWR\nmgwr_betas <- mgwr_model$SDF@data[coef_names] %>%\n    aggregate(by = list(sim$data$group), FUN = mean) %>%\n    as.tibble() %>%\n    select(all_of(coef_names))\n### HLM\nhlm_betas <- coef(hlm_model)$group\ncolnames(hlm_betas)[1] <- \"Intercept\"\nhlm_betas <- hlm_betas[coef_names]\nmodels_coef <- map(models_name, ~ get(paste0(tolower(.x), \"_betas\")))\nnames(models_coef) <- models_name\nmodels_coef_real <- c(list(Real = beta0), models_coef)\n\nThen, plot estimate values at their location and the real values to see their distributions.\n\nmodels_coef_real %>%\n    map_dfr(function(x) {\n        select(x, all_of(coef_names)) %>%\n            map_dfr(~ data.frame(Value = .x, sim$coords), .id = \"Coefficient\")\n    }, .id = \"Algorithm\") %>% \n    mutate(Algorithm = ordered(as.factor(Algorithm), names(models_coef_real)),\n           Coefficient = ordered(as.factor(Coefficient), coef_names, \n                                 labels = coef_name_map[coef_names])) %>%\n    ggplot(aes(u, v, fill = Value)) + geom_raster() + \n        scale_x_continuous(expand = expansion()) +\n        scale_y_continuous(expand = expansion()) +\n        scale_fill_gradient2(limits = c(0, 4), low = \"#000099\", \n                             mid = \"#CCFF33\", high = \"#CC0033\",\n                             midpoint = 2, oob = scales::squish) +\n        facet_grid(rows = vars(Algorithm), cols = vars(Coefficient), labeller = label_parsed) +\n        coord_fixed() + theme_bw() +\n        theme(legend.position = \"bottom\", legend.key.height = unit(10, \"pt\")) \n\n\n\n\nA scatter plot along with it may be useful.\n\nbeta_hat_real <- models_coef %>%\n    map_dfr(function(x) {\n        map_dfr(coef_names, ~ data.frame(Coefficient = .x, Estimated = x[[.x]], Real = beta0[[.x]]))\n    }, .id = \"Algorithm\") %>%\n    mutate(Algorithm = ordered(as.factor(Algorithm), names(models_coef)),\n           Coefficient = ordered(as.factor(Coefficient), coef_names, labels = coef_name_map[coef_names]))\nbeta_hat_rmse <- beta_hat_real %>%\n    group_nest(Algorithm, Coefficient) %>%\n    mutate(rmse = map(data, ~ sqrt(mean((.x$Estimated - .x$Real)^2))),\n           data = NULL)\nggplot(beta_hat_real, aes(x = Real, y = Estimated)) +\n    geom_point() + geom_abline(intercept = 0, slope = 1) +\n    geom_text(aes(x = -Inf, y = Inf, label = sprintf(\"RMSE=%.3f\", rmse)),\n              data = beta_hat_rmse, hjust = 0, vjust = 1.2) +\n    scale_y_continuous(limits = c(-5, 10), oob = scales::squish) +\n    facet_grid(rows = vars(Algorithm), cols = vars(Coefficient), labeller = label_parsed) +\n    theme_bw()\n\n\n\n\nFrom these two figures, we can find that:\n\nIn the results of GWR, spatial heterogeneity is revealed in estimates for all variables. Although \\(\\hat{\\beta}_1\\) should be constant across the study area, GWR still generate spatially varying estimates for it. This is a kind of over-fitting from the spatial perspective. Besides, as the bandwidth is small, estimates for \\(\\gamma_1\\) and \\(\\gamma_2\\) are too local. Consequently, there are quite a few outliers disrupting the spatial trend.\nMGWR partly gets over issues of GWR by adopting parameter-specified bandwidths, instead of a uniform bandwidth. And it performs better when estimating \\(\\gamma_1\\) and \\(\\gamma_2\\). For global fixed effects, MGWR still generates spatially varying estimates, but they vary much slightly than estimates from GWR. For random effects, the results are slightly smoothed as well. MGWR also borrow a few points from neighbors. There is another serious problem in MGWR that it requires too much computing time and memory.\nIn the results of HLM, there is only one estimate for \\(\\beta_1\\) across the whole area. And estimates for \\(\\mu_1\\), The problem lies in estimates for \\(\\gamma_1\\) and \\(\\gamma_2\\). As they are fixed effects in HLM, their estimates are also constant for all samples. However, spatial heterogeneity is expected in them.\nHGWR is the final solution. For global fixed effects, it generates globally constant estimates for all samples. For random effects, it would not smooth the estimates because they are not obtained by borrowing points. And for local fixed effects, we can discover spatial heterogeneity from their estimates. And it would not repeat computation for samples at each location. Thus, only the number of locations obviously affects the computation efficiency. This can reduce a large amount of computing time and memory.\n\nThere is an animation demonstrating the problem about bandwidth we addressed above.\n\nAs shown in this video, bandwidths have inequal spatial scale for two samples (represented by cubes). Both the samples represented by large red cubes and large blue cubes take 41 neighbour samples to calibrate GWR models. For the red one, neighbours on 8 nearest locations are taken. But the figure for the blue one is only 6. This situation means estimated coefficients are more smoothed for the red samples. In other words, estimations for the blue samples are much local.\nHGWR overcome this drawback by introducing hierarchical structure with a special designed backfitting estimator. With the popularity of spatiotemporal big data, situations wherein the specific parameters for which HGWR was optimized are becoming more prevalent, suggesting that HGWR holds considerable promise as a useful tool for analyzing such data sets."
  },
  {
    "objectID": "posts/HGWR/index.html#estimation-errors",
    "href": "posts/HGWR/index.html#estimation-errors",
    "title": "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator",
    "section": "Estimation Errors",
    "text": "Estimation Errors\nWe can use some indicators to evaulate estimation errors for each model and coefficient. The following code generate a box-plot of absolute coefficient errors \\(\\mathrm{AE}\\), which is \\[\n\\mathrm{AE}_i = \\left| r_i - e_i \\right|\n\\] where \\(r_i\\) reprsents the real value at sample \\(i\\), and \\(e_i\\) represents the corresponding estimate.\n\nmodels_coef %>%\n    map(~ select(.x, all_of(coef_names))) %>%\n    map_dfr(function(x) {\n        map2_dfr(x, names(x), ~ data.frame(ae = ae(beta0[[.y]], .x)), .id = \"Coefficient\")\n    }, .id = \"Algorithm\") %>%\n    mutate(Algorithm = ordered(as.factor(Algorithm), names(models_coef)),\n           Coefficient = ordered(as.factor(Coefficient), coef_names)) %>%\n    arrange(Coefficient, Algorithm) %>%\n    ggplot(aes(x = Coefficient, y = ae, fill = Algorithm)) + geom_boxplot() +\n        scale_y_log10(name = 'Squared Error', labels = ~ sprintf(\"%g\", .x)) +\n        scale_x_discrete(labels = coef_name_labels[coef_names]) +\n        theme_bw() +\n        theme(legend.position = \"top\", axis.text.y = element_text(angle = 90, hjust = 0.5))\n\n\n\n\nAnd the following code generate a bar-plot showing root mean squared errors \\(\\mathrm{RMSE}\\) of each coefficient, which is \\[\n\\mathrm{RMSE} = \\sum_{i=1}^n \\left(r_1-e_i\\right)^2\n\\] where \\(r_i\\) reprsents the real value at sample \\(i\\), and \\(e_i\\) represents the corresponding estimate.\n\nmodels_coef %>%\n    map_dfr(function (alg) {\n        map_dfr(coef_names, function (coef, real) {\n            est_rmse <- rmse(real[[coef]], alg[[coef]])\n            data.frame(Coefficient = coef, Value = est_rmse)\n        }, beta0)\n    }, .id = \"Algorithm\") %>%\n    mutate(Algorithm = ordered(as.factor(Algorithm), c(\"GWR\", \"MGWR\", \"HLM\", \"HGWR\")),\n           Coefficient = ordered(as.factor(Coefficient), coef_names)) %>%\n    ggplot(aes(x = Coefficient, y = Value, fill = Algorithm)) + \n        geom_bar(stat = 'identity', position = position_dodge()) + ylab(\"RMSE\") +\n        scale_y_continuous(limits = c(0, 1.6), oob = scales::squish, expand = expansion()) +\n        scale_x_discrete(labels = coef_name_labels[coef_names]) +\n        theme_bw() + theme(legend.position = \"top\")\n\n\n\n\nFrom the perspective of estimation errors, HGWR significantly reduces the estimation error for local fixed effects. It can get over the affect of global fixed effects and random effects."
  },
  {
    "objectID": "posts/DLSM/index.html",
    "href": "posts/DLSM/index.html",
    "title": "Introducing a General Framework for Locally Weighted Spatial Modelling Based on Density Regression",
    "section": "",
    "text": "In this document, I’m going to show codes of simulation experiments and their results demonstrated in the short paper Introducing a General Framework for Locally Weighted Spatial Modelling Based on Density Regression. This paper mainly talks about a density-based local spatial modelling (DLSM) method, which was originally named as “geographically weighted density regression (GWDR)”. In the following parts, we don’t distinguish these two terms.\nIn addition to show reproducable code of experiments shown in the paper, We are going to describe a bit how to install and use this model."
  },
  {
    "objectID": "posts/DLSM/index.html#two-dimensional-data",
    "href": "posts/DLSM/index.html#two-dimensional-data",
    "title": "Introducing a General Framework for Locally Weighted Spatial Modelling Based on Density Regression",
    "section": "Two-dimensional Data",
    "text": "Two-dimensional Data\n\nData Generating\nData of two dimensions (equalivent to normal geographic data) are generated by the following codes.\n\ngenerate_data_d2 <- function (size) {\n  set.seed(11)\n  U1 <- rnorm(n = size, mean = 3000, sd = 100)\n  set.seed(12)\n  U2 <- rnorm(n = size, mean = 3000, sd = 100)\n  set.seed(21)\n  x1 <- rnorm(n = size, mean = 0, sd = 1)\n  set.seed(22)\n  x2 <- rnorm(n = size, mean = 0, sd = 1)\n  set.seed(23)\n  x3 <- rnorm(n = size, mean = 0, sd = 1)\n  U1c <- (U1 - 3000) / 100\n  U2c <- (U2 - 3000) / 100\n  b0 <- U1c + U2c^2\n  b1 <- U1c + U2c^2 + 10\n  b2 <- U1c + (U2c - 1)^2\n  b3 <- U1c + U2c^2 + 2 * U2c\n  set.seed(1)\n  y <- b0 + b1 * x1 + b2 * x2 + b3 * x3 + rnorm(n = size, mean = 0, sd = 1)\n  list(\n    data = data.frame(y = y, x1 = x1, x2 = x2, x3 = x3),\n    coords = cbind(U1 = U1, U2 = U2),\n    beta = data.frame(Intercept = b0, x1 = b1, x2 = b2, x3 = b3)\n  )\n}\ndata_d2 <- generate_data_d2(5000)\nglimpse(data_d2)\n\nList of 3\n $ data  :'data.frame': 5000 obs. of  4 variables:\n  ..$ y : num [1:5000] 7.06 7.66 15.01 -14.52 37.88 ...\n  ..$ x1: num [1:5000] 0.793 0.522 1.746 -1.271 2.197 ...\n  ..$ x2: num [1:5000] -0.512 2.485 1.008 0.293 -0.209 ...\n  ..$ x3: num [1:5000] 0.193 -0.435 0.913 1.793 0.997 ...\n $ coords: num [1:5000, 1:2] 2941 3003 2848 2864 3118 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"U1\" \"U2\"\n $ beta  :'data.frame': 5000 obs. of  4 variables:\n  ..$ Intercept: num [1:5000] 1.601 2.514 -0.601 -0.516 5.169 ...\n  ..$ x1       : num [1:5000] 11.6 12.51 9.4 9.48 15.17 ...\n  ..$ x2       : num [1:5000] 5.56 0.36 2.31 2.32 10.16 ...\n  ..$ x3       : num [1:5000] -1.36 5.67 -2.51 -2.36 1.17 ...\n\n\nThen, calibrate two models: DLSM and basic GWR.\n\n\nModel: DLSM\nFirstly, we need to get a set of optimized bandwidth, each element for a dimension.\n\nd2_gwdr_bw <- gwdr.bandwidth.optimize(\n    formula = y ~ x1 + x2 + x3,\n    data = data_d2$data,\n    coords = data_d2$coords,\n    kernel.list = list(\n        gwdr.make.kernel(0.618, kernel = \"gaussian\", adaptive = T),\n        gwdr.make.kernel(0.618, kernel = \"gaussian\", adaptive = T)\n    ),\n    optimize.method = gwdr.bandwidth.optimize.aic\n)\nd2_gwdr_bw\n\n[[1]]\n[[1]][[1]]\n[1] \"gaussian\"\n\n[[1]][[2]]\n[1] 0.2243633\n\n[[1]][[3]]\n[1] TRUE\n\n\n[[2]]\n[[2]][[1]]\n[1] \"gaussian\"\n\n[[2]][[2]]\n[1] 0.008293663\n\n[[2]][[3]]\n[1] TRUE\n\n\nThen, calibrate a GWDR model with this bandwidth set.\n\nd2_gwdr <- gwdr(\n    formula = y ~ x1 + x2 + x3,\n    data = data_d2$data,\n    coords = data_d2$coords,\n    kernel.list = d2_gwdr_bw\n)\nd2_gwdr$diagnostic\n\n$R2\n[1] 0.9890828\n\n$R2.adj\n[1] 0.9838953\n\n$AICc\n[1] 19695.4\n\n\n\n\nModel: GWR\nThe GWR model for this data set can be calibrated with the following code.\n\nd2sp <- data_d2$data\ncoordinates(d2sp) <- data_d2$coords\nd2_gwr_bw <- bw.gwr(\n    formula = y ~ x1 + x2 + x3,\n    data = d2sp,\n    adaptive = T,\n    approach = \"AIC\",\n    kernel = \"gaussian\",\n    longlat = F\n)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth (number of nearest neighbours): 3097 AICc value: 28564.75 \nAdaptive bandwidth (number of nearest neighbours): 1922 AICc value: 27977.58 \nAdaptive bandwidth (number of nearest neighbours): 1194 AICc value: 27333.17 \nAdaptive bandwidth (number of nearest neighbours): 746 AICc value: 26614.68 \nAdaptive bandwidth (number of nearest neighbours): 467 AICc value: 25821.45 \nAdaptive bandwidth (number of nearest neighbours): 296 AICc value: 24986.24 \nAdaptive bandwidth (number of nearest neighbours): 189 AICc value: 24106.2 \nAdaptive bandwidth (number of nearest neighbours): 124 AICc value: 23222.91 \nAdaptive bandwidth (number of nearest neighbours): 82 AICc value: 22314.04 \nAdaptive bandwidth (number of nearest neighbours): 58 AICc value: 21556.79 \nAdaptive bandwidth (number of nearest neighbours): 41 AICc value: 20825.54 \nAdaptive bandwidth (number of nearest neighbours): 32 AICc value: 20268.46 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 19794.13 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 19582.89 \nAdaptive bandwidth (number of nearest neighbours): 19 AICc value: 19302.27 \nAdaptive bandwidth (number of nearest neighbours): 18 AICc value: 19213.83 \nAdaptive bandwidth (number of nearest neighbours): 16 AICc value: 18980.64 \nAdaptive bandwidth (number of nearest neighbours): 16 AICc value: 18980.64 \n\nd2_gwr <- gwr.basic(\n    formula = y ~ x1 + x2 + x3,\n    data = d2sp,\n    bw = d2_gwr_bw,\n    adaptive = T,\n    kernel = \"gaussian\",\n    longlat = F\n)\nd2_gwr\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-03-27 16:34:17 \n   Call:\n   gwr.basic(formula = y ~ x1 + x2 + x3, data = d2sp, bw = d2_gwr_bw, \n    kernel = \"gaussian\", adaptive = T, longlat = F)\n\n   Dependent (y) variable:  y\n   Independent variables:  x1 x2 x3\n   Number of data points: 5000\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-41.449  -2.388  -0.283   1.964  35.784 \n\n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)  1.19760    0.06607   18.13   <2e-16 ***\n   x1          11.12053    0.06613  168.17   <2e-16 ***\n   x2           2.08131    0.06610   31.49   <2e-16 ***\n   x3           1.16391    0.06631   17.55   <2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 4.671 on 4996 degrees of freedom\n   Multiple R-squared: 0.8557\n   Adjusted R-squared: 0.8556 \n   F-statistic:  9874 on 3 and 4996 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 108997.5\n   Sigma(hat): 4.669928\n   AIC:  29608.82\n   AICc:  29608.83\n   BIC:  24683.99\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 16 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                   Min.    1st Qu.     Median    3rd Qu.    Max.\n   Intercept -2.3010764  0.0021137  0.8763939  1.7517336  6.1792\n   x1         7.2879239 10.0490656 10.7414589 11.7858821 16.3170\n   x2        -1.9011456  0.2368896  1.3907690  3.0352093 12.2075\n   x3        -2.9867908 -0.6821843  0.3649609  2.1155378  9.8081\n   ************************Diagnostic information*************************\n   Number of data points: 5000 \n   Effective number of parameters (2trace(S) - trace(S'S)): 882.6782 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 4117.322 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 18980.64 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 18192.84 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 17803.57 \n   Residual sum of squares: 9849.866 \n   R-square value:  0.9869581 \n   Adjusted R-square value:  0.9841614 \n\n   ***********************************************************************\n   Program stops at: 2023-03-27 16:34:25 \n\n\nWhereas DLSM helps identify anisotropy, it is missing in estimates from a basic GWR model because the only bandwidth value optimized by GWR is 16 nearest neighbours (regardless of direction).\n\n\nAnalysis of Coefficient Estimates\nFirst, we look at the closeness between coefficient estimates and actual values.\n\nlist(DLSM = d2_gwdr$betas, GWR = d2_gwr$SDF@data) %>%\n    map(~ select(.x, Intercept, x1, x2, x3)) %>%\n    map2_dfr(., names(.), function(model, model_name) {\n        map_dfr(c(\"Intercept\", \"x1\", \"x2\", \"x3\"), ~ data.frame(\n            Estimated = model[[.x]],\n            Real = data_d2$beta[[.x]],\n            Coefficient = .x\n        ))\n    }, .id = \"Model\") %>%\n    ggplot(aes(x = Real, y = Estimated)) + geom_point() +\n    geom_abline(intercept = 0, slope = 1, color = \"darkgreen\") +\n    stat_poly_eq() + stat_poly_line() +\n    facet_grid(rows = vars(Model), cols = vars(Coefficient)) +\n    coord_fixed() + theme_bw()\n\n\n\n\nThen, we look at the RMSE and MAE criterions.\n\nlist(DLSM = d2_gwdr$betas, GWR = d2_gwr$SDF@data) %>%\n    map(~ select(.x, Intercept, x1, x2, x3)) %>%\n    map2_dfr(., names(.), function(model, model_name) {\n        map_dfr(c(\"Intercept\", \"x1\", \"x2\", \"x3\"), ~ data.frame(\n            RMSE = sqrt(mean((data_d2$beta[[.x]] - model[[.x]])^2)),\n            MAE = mean(abs(data_d2$beta[[.x]] - model[[.x]])),\n            Coefficient = .x\n        ))\n    }, .id = \"Model\") %>%\n    map_dfr(c(\"RMSE\", \"MAE\"), function(i, model) {\n        data.frame(Value = model[[i]],\n                   Indicator = i,\n                   Model = model$Model,\n                   Coefficient = model$Coefficient)\n    }, .) %>%\n    ggplot(aes(x = Coefficient, y = Value, fill = Model)) + \n    geom_col(position = \"dodge\") +\n    geom_text(aes(y = Value + 0.02, label = sprintf(\"%.2f\", Value)),\n              position = position_dodge(width = 1)) +\n    facet_grid(cols = vars(Indicator)) +\n    theme_bw() + theme(legend.position = \"top\")\n\n\n\n\n\n\nLocal Polynomial Estimator\nCoefficient estimates for some points are significantly biased in both DLSM and GWR models. Now let us try the local polynomial kernel estimation method to demonstrate some of its features. We will calibrate a DLSM model with this kernel analyse coefficient estimates in a same way.\n\nd2_gwdr_lp <- gwdr(\n    formula = y ~ x1 + x2 + x3,\n    data = data_d2$data,\n    coords = data_d2$coords,\n    kernel.list = d2_gwdr_bw,\n    solver = \"local.poly\"\n)\nd2_gwdr$diagnostic\n\n$R2\n[1] 0.9890828\n\n$R2.adj\n[1] 0.9838953\n\n$AICc\n[1] 19695.4\n\n\nThe following two figures show comparsion between estimates and real values.\n\n\n\n\n\n\n\n\nThus, the local polynomial estimator can significantly reduce estimation errors. And the boundary effects are also reduced."
  },
  {
    "objectID": "posts/DLSM/index.html#three-dimensional-data",
    "href": "posts/DLSM/index.html#three-dimensional-data",
    "title": "Introducing a General Framework for Locally Weighted Spatial Modelling Based on Density Regression",
    "section": "Three-dimensional Data",
    "text": "Three-dimensional Data\nIn most spatial modelling research, 3D data are usually referred to spatio-temporal data, i.e., data of geographical and temporal coordinates \\(u,v,t\\). For this type of data, there is a corresponding geographically and temporally weighted regression (GTWR, Huang, Wu, and Barry 2010) model. In this experiment, we compare DLSM model with this method.\n\nData\nWe created 4 sets of data through similar generation process introduced in the experiment on 2D data, named as compare-gtwr-i.rds where i is a value from 1 to 4. To access these data, please turn to GitHub worktree page.\n\nd3_data_list <- map(c(1:4), function(i) {\n    readRDS(sprintf(\"data/compare-gtwr-%d.rds\", i))\n})\n\nIn the first two data sets, the time coordinates were generated from a normal distributed random variable, i.e., \\(t \\sim N(1619694000, 604800^2)\\). While in the latter two data sets, \\(t\\) was generated from an arithmetic sequence with 1000 elements, a common different of 1, and a first item \\(t_0\\) of \\(1619694000\\). And the distribution of coefficients on \\(t\\)-axis follows autoregressive time series.\n\n\nModel: DLSM\nThe DLSM model can be calibrated with the following codes:\n\nd3_gwdr_list <- map(c(1:4), function (i) {\n    d3_data <- d3_data_list[[i]]\n    coords_range <- apply(d3_data$coord, 2, max) - apply(d3_data$coord, 2, min)\n    kernel <- gwdr.bandwidth.optimize(\n        formula = y ~ x1 + x2 + x3,\n        data = d3_data$data,\n        coords = d3_data$coord,\n        kernel.list = list(\n            gwdr.make.kernel(coords_range[1] * 0.618, kernel = \"bisquare\", adaptive = FALSE),\n            gwdr.make.kernel(coords_range[2] * 0.618, kernel = \"bisquare\", adaptive = FALSE),\n            gwdr.make.kernel(coords_range[3] * 0.618, kernel = \"bisquare\", adaptive = FALSE)\n        )\n    )\n    gwdr(\n        formula = y ~ x1 + x2 + x3,\n        data = d3_data$data,\n        coords = d3_data$coord,\n        kernel.list = kernel\n    )\n})\n\n\n\nModel: GTWR\nWe used the “GTWR ADDIN” for ArcMap (Huang and Wang 2020) to calibrate GTWR model for all the four data sets. This is because there is a key parameter \\(\\lambda\\) in GTWR mdoel which should be optimized according to data, just like the bandwidth. But gtwr() function in GWmodel package does not support this process. And this addin has much higher computing performance. Results are stored in the GTWR results folder. We can load them with the following codes.\n\nd3_gtwr_list <- map(c(1:4), function(i) {\n    st_read(file.path(\"gtwr_results\", sprintf(\"compare-gtwr-%d-gtwr.shp\", i)))\n})\n\nReading layer `compare-gtwr-1-gtwr' from data source \n  `C:\\Users\\rd21411\\OneDrive - University of Bristol\\Documents\\Conferences\\2023_GIScience_Materials\\posts\\DLSM\\gtwr_results\\compare-gtwr-1-gtwr.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1000 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2672.301 ymin: 2695.423 xmax: 3369.969 ymax: 3310.724\nCRS:           NA\nReading layer `compare-gtwr-2-gtwr' from data source \n  `C:\\Users\\rd21411\\OneDrive - University of Bristol\\Documents\\Conferences\\2023_GIScience_Materials\\posts\\DLSM\\gtwr_results\\compare-gtwr-2-gtwr.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1000 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2672.301 ymin: 2695.423 xmax: 3369.969 ymax: 3310.724\nCRS:           NA\nReading layer `compare-gtwr-3-gtwr' from data source \n  `C:\\Users\\rd21411\\OneDrive - University of Bristol\\Documents\\Conferences\\2023_GIScience_Materials\\posts\\DLSM\\gtwr_results\\compare-gtwr-3-gtwr.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1000 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2672.301 ymin: 2695.423 xmax: 3369.969 ymax: 3310.724\nCRS:           NA\nReading layer `compare-gtwr-4-gtwr' from data source \n  `C:\\Users\\rd21411\\OneDrive - University of Bristol\\Documents\\Conferences\\2023_GIScience_Materials\\posts\\DLSM\\gtwr_results\\compare-gtwr-4-gtwr.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1000 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2672.301 ymin: 2695.423 xmax: 3369.969 ymax: 3310.724\nCRS:           NA\n\n\n\n\nAnalysis\nWe analyse the performance of these two models based on coefficient estimates and actual values.\n\nd3_model_coef <- pmap(list(\n    DLSM = d3_gwdr_list,\n    GTWR = d3_gtwr_list,\n    Real = d3_data_list\n), function(DLSM, GTWR, Real) {\n    dlsm_coef_df <- select(DLSM$betas, Intercept, x1, x2, x3) %>%\n        map2_dfr(., names(.), ~ data.frame(\n            Model = \"DLSM\",\n            Coefficient = .y,\n            Estimate = .x,\n            Real = Real$beta[[.y]]\n        ))\n    gtwr_coef_df <- rename(GTWR, x1 = C1_x1, x2 = C2_x2, x3 = C3_x3) %>%\n        st_drop_geometry() %>%\n        select(Intercept, x1, x2, x3) %>%\n        map2_dfr(., names(.), ~ data.frame(\n            Model = \"GTWR\",\n            Coefficient = .y,\n            Estimate = .x,\n            Real = Real$beta[[.y]]\n        ))\n    rbind(dlsm_coef_df, gtwr_coef_df)\n})\n\n\nd3_model_coef %>%\n    map(function(item) {\n        scatter <- ggplot(item, aes(Real, Estimate)) +\n            geom_point() +\n            geom_abline(intercept = 0, slope = 1) +\n            geom_smooth(method = \"lm\") +\n            stat_poly_eq(use_label(\"adj.rr.label\")) +\n            facet_grid(rows = vars(Coefficient), cols = vars(Model)) +\n            coord_fixed() + theme_bw()\n        bar <- item %>%\n            group_by(Coefficient, Model) %>%\n            summarise(RMSE = rmse(Real, Estimate), MAE = mae(Real, Estimate)) %>%\n            ungroup() %>%\n            melt(id.vars = c(\"Coefficient\", \"Model\"), variable.name = \"Indicator\", value.name = \"Value\") %>%\n            ggplot(aes(Coefficient, Value, fill = Model)) +\n                geom_col(position = \"dodge\") +\n                geom_text(aes(label = sprintf(\"%.2f\", Value)), size = 2,\n                        position = position_dodge(1), vjust = -0.5) +\n                facet_grid(rows = vars(Indicator)) +\n                theme_bw()\n        ggarrange(scatter, bar, nrow = 1)\n    }) %>%\n    walk2(., 1:4, function(fig, i) {\n        print(annotate_figure(fig, bottom = sprintf(\"Data set %d\", i)))\n    })\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the results, DLSM can reduce the mean of absolute estimation error by 10%-50%, especially when coefficients are temporally autocorrelated. The multiple bandwidths attach actual meaning to the parameters \\(\\lambda,\\mu\\); they have a real-world correlate, unlike the root of sum of squared meters and seconds (\\(\\sqrt{\\mathrm{m}^2+\\mathrm{s}^2}\\))."
  },
  {
    "objectID": "posts/DLSM/index.html#four-dimensional-data",
    "href": "posts/DLSM/index.html#four-dimensional-data",
    "title": "Introducing a General Framework for Locally Weighted Spatial Modelling Based on Density Regression",
    "section": "Four-dimensional Data",
    "text": "Four-dimensional Data\nFour-dimensional data are not so common in our daily life. But there is an special example — travel flow. Each flow is a directed line consisting of a origin point and destination point. Thus, flow data are also called O-D data. For a 2D coordinate reference system, both the origin point and desitnation point have a 2D coordinates. Totally, there are 4 coordinates to locate a flow. By converting the four positional coordinates to a set of coordinates of origin point \\((x,y)\\), direction \\(\\theta\\), and flow length \\(l\\), we will get a space of four dimensions \\((x,y,\\theta,l)\\). The experiment is based on this space.\n\nData Generating\nData of four dimensions are generated by the following codes.\n\ngenerate_data_d4 <- function (size) {\n  set.seed(11)\n  U1 <- rnorm(n = size, mean = 3000, sd = 100)\n  set.seed(12)\n  U2 <- rnorm(n = size, mean = 3000, sd = 100)\n  set.seed(13)\n  U3 <- runif(n = size, min = -pi, max = pi)\n  set.seed(14)\n  U4 <- rnorm(n = size, mean = 4000, sd = 1000)\n  set.seed(21)\n  x1 <- rnorm(n = size, mean = 0, sd = 1)\n  set.seed(22)\n  x2 <- rnorm(n = size, mean = 0, sd = 1)\n  set.seed(23)\n  x3 <- rnorm(n = size, mean = 0, sd = 1)\n  b0 <- scale(((U1 - 3000)/100) + ((U2 - 3000)/100)^2 + ((U4 - 4000)/1000)^2)\n  b1 <- scale(((U1 - 3000)/100) + ((U2 - 3000)/100)^2 + ((U4 - 4000)/1000)^2)\n  b2 <- scale(((U1 - 3000)/100) + 5 * ((U2 - 3000)/100)^2 + ((U4 - 4000)/1000)^2)\n  b3 <- scale(-((U1 - 3000)/100) + ((U2 - 3000)/100)^2 + ((U4 - 4000)/1000)^2)\n  set.seed(1)\n  y <- b0 + b1 * x1 + b2 * x2 + b3 *x3 + rnorm(n = size, mean = 0, sd = 1)\n  list(\n    data = data.frame(y = y, x1 = x1, x2 = x2, x3 = x3),\n    coords = cbind(U1 = U1, U2 = U2, U3 = U3, U4 = U4),\n    beta = data.frame(Intercept = b0, x1 = b1, x2 = b2, x3 = b3)\n  )\n}\ndata_d4 <- generate_data_d4(5000)\n\n\n\nModel: DLSM\nThe DLSM model can be calibrated by the following code.\n\nd4_gwdr_bw <- gwdr.bandwidth.optimize(\n    formula = y ~ x1 + x2 + x3,\n    data = data_d4$data,\n    coords = data_d4$coords,\n    kernel.list = list(\n        gwdr.make.kernel(0.618, kernel = \"bisquare\", adaptive = T),\n        gwdr.make.kernel(0.618, kernel = \"bisquare\", adaptive = T),\n        gwdr.make.kernel(0.618, kernel = \"bisquare\", adaptive = T),\n        gwdr.make.kernel(0.618, kernel = \"bisquare\", adaptive = T)\n    )\n)\nd4_gwdr <- gwdr(\n    formula = y ~ x1 + x2 + x3,\n    data = data_d4$data,\n    coords = data_d4$coords,\n    kernel.list = d4_gwdr_bw\n)\nd4_gwdr$diagnostic\n\n$R2\n[1] 0.8242963\n\n$R2.adj\n[1] 0.7287304\n\n$AICc\n[1] 17255.57\n\n\n\n\nModel: GWR\nTo calibrate a GWR model, we need to calculate the distance matrix first because gwr.basic() is not able to calculate distances for lines. Distance between two flows \\(\\overrightarrow{O_iD_i}\\) and \\(\\overrightarrow{O_jD_j}\\) are defined by Kordi and Fotheringham (2016), \\[\nd_{ij}=\\sqrt{\\frac{\n    0.5 \\times \\left[ (O_{ix}-O_{jx})^2 + (O_{iy}-O_{jy})^2 \\right] +\n    0.5 \\times \\left[ (D_{ix}-D_{jx})^2 + (D_{iy}-D_{jy})^2 \\right]\n}{l_i l_j}}\n\\] where \\((O_{ix},O_{iy})\\) is the coordinate of \\(O_i\\), \\((O_{jx},O_{jy})\\) is the coordinate of \\(O_j\\), \\((D_{ix},D_{iy})\\) is the coordinate of \\(D_i\\), \\((D_{jx},D_{jy})\\) is the coordinate of \\(D_j\\), and \\(l_i,l_j\\) are length of flows \\(\\overrightarrow{O_iD_i}\\) and \\(\\overrightarrow{O_jD_j}\\).\nThis is implementated by the following codes.\n\nd4_origin <- data_d4$coords[, 1:2]\nd4_dest <- d4_origin + with(as.data.frame(data_d4$coords), matrix(cbind(U4 * cos(U3), U4 * sin(U3)), ncol = 2))\nd4_od <- cbind(d4_origin, d4_dest, data_d4$coords[, 3:4])\ncolnames(d4_od) <- c(\"ox\", \"oy\", \"dx\", \"dy\", \"angle\", \"length\")\nd4_dmat <- apply(d4_od, MARGIN = 1, FUN = function(x) {\n    sqrt(colSums((t(d4_od[,1:4]) - x[1:4])^2) / 2 / x[\"length\"] / d4_od[, \"length\"])\n})\nd4_dmat[1:5,1:5]\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 0.0000000 1.4077782 1.2585313 1.3656097 0.9873034\n[2,] 1.4077782 0.0000000 0.5916647 0.6618936 1.0783478\n[3,] 1.2585313 0.5916647 0.0000000 1.1402567 1.3717634\n[4,] 1.3656097 0.6618936 1.1402567 0.0000000 0.5994136\n[5,] 0.9873034 1.0783478 1.3717634 0.5994136 0.0000000\n\n\nThen use the distance matrix d4_dmat as weighting criterion in GWR.\n\nd4sp <- cbind(data_d4$data)\ncoordinates(d4sp) <- data_d4$coords[,1:2]\nd4_gwr_bw <- bw.gwr(\n    formula = y ~ x1 + x2 + x3, data = d4sp,\n    adaptive = T, dMat = d4_dmat\n)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 3097 CV score: 27063.63 \nAdaptive bandwidth: 1922 CV score: 26809.04 \nAdaptive bandwidth: 1194 CV score: 26172.95 \nAdaptive bandwidth: 746 CV score: 25282.31 \nAdaptive bandwidth: 467 CV score: 24435.03 \nAdaptive bandwidth: 296 CV score: 23756.82 \nAdaptive bandwidth: 189 CV score: 23263.05 \nAdaptive bandwidth: 124 CV score: 22945.49 \nAdaptive bandwidth: 82 CV score: 22825.49 \nAdaptive bandwidth: 58 CV score: 22944.28 \nAdaptive bandwidth: 98 CV score: 22847.02 \nAdaptive bandwidth: 72 CV score: 22835.85 \nAdaptive bandwidth: 87 CV score: 22844.34 \nAdaptive bandwidth: 77 CV score: 22829.18 \nAdaptive bandwidth: 83 CV score: 22829.85 \nAdaptive bandwidth: 79 CV score: 22825.71 \nAdaptive bandwidth: 81 CV score: 22821.32 \nAdaptive bandwidth: 83 CV score: 22829.85 \nAdaptive bandwidth: 82 CV score: 22825.49 \nAdaptive bandwidth: 83 CV score: 22829.85 \nAdaptive bandwidth: 82 CV score: 22825.49 \nAdaptive bandwidth: 82 CV score: 22825.49 \nAdaptive bandwidth: 81 CV score: 22821.32 \n\nd4_gwr <- gwr.basic(\n    formula = y ~ x1 + x2 + x3, data = d4sp,\n    bw = d4_gwr_bw, adaptive = T, dMat = d4_dmat\n)\nd4_gwr\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-03-28 17:49:16 \n   Call:\n   gwr.basic(formula = y ~ x1 + x2 + x3, data = d4sp, bw = d4_gwr_bw, \n    adaptive = T, dMat = d4_dmat)\n\n   Dependent (y) variable:  y\n   Independent variables:  x1 x2 x3\n   Number of data points: 5000\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-19.6256  -1.2957  -0.1548   1.0775  28.6380 \n\n   Coefficients:\n                Estimate Std. Error t value Pr(>|t|)   \n   (Intercept) -0.003055   0.032933  -0.093  0.92609   \n   x1           0.054850   0.032961   1.664  0.09616 . \n   x2           0.093825   0.032946   2.848  0.00442 **\n   x3           0.062644   0.033052   1.895  0.05811 . \n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 2.328 on 4996 degrees of freedom\n   Multiple R-squared: 0.002916\n   Adjusted R-squared: 0.002317 \n   F-statistic:  4.87 on 3 and 4996 DF,  p-value: 0.002203 \n   ***Extra Diagnostic information\n   Residual sum of squares: 27080.44\n   Sigma(hat): 2.327715\n   AIC:  22646.25\n   AICc:  22646.27\n   BIC:  17721.43\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidth: 81 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: A distance matrix is specified for this model calibration.\n\n   ****************Summary of GWR coefficient estimates:******************\n                  Min.   1st Qu.    Median   3rd Qu.   Max.\n   Intercept -1.429833 -0.486051 -0.203637  0.148622 2.9955\n   x1        -1.110369 -0.443654 -0.171952  0.183283 4.6331\n   x2        -1.010210 -0.334072 -0.071004  0.203844 3.4854\n   x3        -1.396231 -0.456705 -0.169986  0.212258 3.3622\n   ************************Diagnostic information*************************\n   Number of data points: 5000 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1119.033 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 3880.967 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 21308.13 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 20202.63 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 21205.94 \n   Residual sum of squares: 14187.55 \n   R-square value:  0.4776237 \n   Adjusted R-square value:  0.3269636 \n\n   ***********************************************************************\n   Program stops at: 2023-03-28 17:49:20 \n\n\n\n\nAnalysis\nCloseness between coefficient estimates and actual values are shown in the following figure.\n\nlist(DLSM = d4_gwdr$betas, GWR = d4_gwr$SDF@data) %>%\n    map(~ select(.x, Intercept, x1, x2, x3)) %>%\n    map2_dfr(., names(.), function(model, model_name) {\n        map_dfr(c(\"Intercept\", \"x1\", \"x2\", \"x3\"), ~ data.frame(\n            Estimated = model[[.x]],\n            Real = data_d4$beta[[.x]],\n            Model = model_name,\n            Coefficient = .x\n        ))\n    }) %>%\n    ggplot(aes(x = Real, y = Estimated)) + geom_point() +\n    geom_abline(intercept = 0, slope = 1, color = \"darkgreen\") +\n    stat_poly_eq() + stat_poly_line() +\n    facet_grid(rows = vars(Model), cols = vars(Coefficient)) +\n    theme_bw()\n\n\n\n\nRMSE and MAE evaluations are shown in the following figure.\n\nlist(DLSM = d4_gwdr$betas, GWR = d4_gwr$SDF@data) %>%\n    map(~ select(.x, Intercept, x1, x2, x3)) %>%\n    map2_dfr(., names(.), function(model, model_name) {\n        map_dfr(c(\"Intercept\", \"x1\", \"x2\", \"x3\"), ~ data.frame(\n            RMSE = sqrt(mean((data_d4$beta[[.x]] - model[[.x]])^2)),\n            MAE = mean(abs(data_d4$beta[[.x]] - model[[.x]])),\n            Model = model_name,\n            Coefficient = .x\n        ))\n    }) %>%\n    map_dfr(c(\"RMSE\", \"MAE\"), function(i, model) {\n        data.frame(Value = model[[i]],\n                   Indicator = i,\n                   Model = model$Model,\n                   Coefficient = model$Coefficient)\n    }, .) %>%\n    ggplot(aes(x = Coefficient, y = Value, fill = Model)) + \n    geom_col(position = \"dodge\") +\n    geom_text(aes(y = Value + 0.02, label = sprintf(\"%.2f\", Value)),\n              position = position_dodge(width = 1)) +\n    facet_grid(cols = vars(Indicator)) +\n    theme_bw() + theme(legend.position = \"top\")\n\n\n\n\nResults show that DLSM works well for spatial line data even without defining distance metrics. It performs better than GWR according to mean of estimation errors, but a few outliers exist in estimates. GWR selected a much smaller bandwidth (173 neighbours). Thus, the risk of overfitting reappears."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Contents",
    "section": "",
    "text": "A Hierarchical and Geographically Weighted Regression Model and Its Backfitting Maximum Likelihood Estimator\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nYigong Hu\n\n\n\n\n\n\n\n\nIntroducing a General Framework for Locally Weighted Spatial Modelling Based on Density Regression\n\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nYigong Hu\n\n\n\n\n\n\nNo matching items"
  }
]